{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chris Kroenke's Site"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning Embedding Models with Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are finally at the point that many people have been waiting for: small LLMs have become quite powerful and can run on consumer GPUs. With good fine-tuning in a given domain, they even rival some of the best commercially available LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This combo of being runnable and fine-tuneable on consumer hardware is possible thanks to weight quantization and LoRA adapters, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This post fine-tunes a text embedding model with the unsloth and Sentence Transformers libraries. Specifically, we fine-tune a set of QLoRA adapters using a contrastive loss on a simple Question and Answer dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The unsloth library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The unsloth library makes it both efficient and affordable to fine-tune transformer networks on consumer hardware."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unsloth has an ocean of starter notebooks that make it easy for anyone to fine-tune relevant, modern LLMs. Many of the notebooks use quantization setups that even fit on 8GB GPUs. If you went back a few years ago, and told people we'd be able to meaningfully fine-tune powerful, SoTA LLMs on such small cards it would have sounded outlandish."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Most of their work focuses on fine-tuning decoder models, aka the LLM family of models. This makes sense given the high visibility and ever-increasing capabilities of generative networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While generative LLMs receive much attention, there is also the flip side of the architecture coin: encoder models. These are models like BERT that transform sentences into vector embeddings that capture semantic content and relationships."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Encoder models power incredibly useful tools like RAG. Despite the LLM hype, it is RAG engines that are the backbone of most LLM applications currently deployed in the wild."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG workhorses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RAG engines rely on text embedding models, aka the encoder side of transformer networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is a great post here from the creators of the recent modernBERT model that describes how LLMs capture all the hype and fanfare, but encoding models are the actual workhorses for AI products."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unfortunately, as of writing, unsloth does not directly support fine-tuning encoder models. It's been a feature in their pipeline for a while, but they understandably have a ton of other pressing work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can still however leverage some recent PRs, along with the Sentence Transformers library, to patch fine-tuning embeddings into unsloth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this post, we will fine-tune an all-MiniLM model, specifically the recent all-MiniLM-L12-v2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tuning embeddings with unsloth and Sentence Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's describe the overall process we'll go through. We first take the all-MiniLM model and wrap it in unsloth's QLoRA adapters. Then, we again wrap the unsloth-patched model inside of a custom Sentence Transformers model. It is this final double-wrapped model that will be fine-tuned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Both Sentence Transformers and unsloth actually subclass HuggingFace's Trainer and TrainingArguments. Their APIs and functionality aren't quite identical, but are close enough for our purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unsloth will handle the QLoRA adapters that make it possible to fine-tune encoder models with a tiny fraction of the parameters that full fine-tuning would have taken."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sentence Transformers will do the heavy lifting of the learning loop: preparing the input batches, computing the embeddings-specific loss, and handling the weight updates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's get started and put all of this together. First, we need to prepare our environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installing Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following command installs unsloth:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install unsloth\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that unsloth is under constant development. It directly patches and modifies many low-level libraries used for LLM inference and training. Because of this, it can be quite tricky to install. The default setup in their Google Colab notebooks do a specific pip installation dance that is quite handy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You may have good luck with the simple pip install unsloth. Depending on your linux setup, it might not be so simple. If the simple install fails, mimic the Colab-specific pip installation below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# only do this if the simple pip install fails\n",
        "pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "pip install --no-deps unsloth\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once this is ready, we can import unsloth and get started."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import unsloth first so it can patch in optimizations\n",
        "import unsloth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\ud83e\udda5 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
        "\ud83e\udda5 Unsloth Zoo will now patch everything to make training faster!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that it's best-practice to import unsloth before anything else. This lets it patch all the lower-level libraries that it needs. Then we'll be using the FastModel class. This class takes a very handy auto_class argument that lets us load the actual encoder models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# loads encoder models\n",
        "from unsloth import FastModel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can bring in all of our regular imports. We import all of them here for convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# general imports\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "# import the huggingface classes\n",
        "from transformers import BertModel\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from peft import LoraConfig, TaskType\n",
        "\n",
        "# import the sentence transformers classes\n",
        "import sentence_transformers\n",
        "from sentence_transformers import SentenceTransformerTrainingArguments, SentenceTransformerTrainer, SentenceTransformer\n",
        "from sentence_transformers.util import cos_sim\n",
        "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
        "from sentence_transformers.training_args import BatchSamplers\n",
        "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now start setting the variables we'll need. As mentioned, we're using the all-MiniLM-L12-v2 model which is part of the BERT family."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Configuration\n",
        "BASE_MODEL_ID = 'sentence-transformers/all-MiniLM-L12-v2'\n",
        "BERT_MODEL = BertModel\n",
        "\n",
        "# Maximum sequence length of this model\n",
        "MAX_SEQ_LENGTH = 512\n",
        "LOAD_IN_4BIT = True  # For QLoRA (4-bit quantization)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's load this using FastModel. Note that this is the full model, before we've attached any QLoRA adapters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the base model optimized with unsloth\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = BASE_MODEL_ID,\n",
        "    auto_model = BERT_MODEL,\n",
        "    max_seq_length = MAX_SEQ_LENGTH,\n",
        "    dtype = None, # Auto-detects (BF16/FP16)\n",
        "    load_in_4bit = LOAD_IN_4BIT,\n",
        ")\n",
        "print(f\"Loaded {BASE_MODEL_ID} with Unsloth.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "==((====))==  Unsloth 2025.4.7: Fast Bert patching. Transformers: 4.51.3.\n",
        "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
        "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
        "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
        " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
        "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
        "Loaded sentence-transformers/all-MiniLM-L12-v2 with Unsloth.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## QLoRA patches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we'll attach the QLoRA weights to be learned using  unsloth. Unsloth has a whole set of good, hard-won default arguments for fine-tuning LLMs. From my initial experiments, it seems like some of these will need re-thinking for encoder models. But, they are certainly a solid starting point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LORA Configuration\n",
        "LORA_R = 16          # Rank of the LORA matrices.\n",
        "LORA_ALPHA = 32      # Rule of thumb: 2 * rank\n",
        "LORA_DROPOUT = 0.0   # Dropout of 0 is best.\n",
        "USE_RSLORA = False   # Rank-Stabilized LoRA if desired\n",
        "\n",
        "# Target modules for adapters\n",
        "LORA_TARGET_MODULES = [\"query\", \"key\", \"value\", \"dense\"]\n",
        "LORA_EXCLUDE_MODULES = [] # put anything you want to skip here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With our QLoRA settings, we can then attach them to the base model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Attaching QLoRA adapters...\")\n",
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    r = LORA_R,\n",
        "    lora_alpha = LORA_ALPHA,\n",
        "    lora_dropout = LORA_DROPOUT,\n",
        "    target_modules = LORA_TARGET_MODULES,\n",
        "    exclude_modules = LORA_EXCLUDE_MODULES,\n",
        "    use_rslora = USE_RSLORA,\n",
        "    bias = \"none\", # Standard practice for LoRA\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    modules_to_save = None, # Add to train non-LORA modules\n",
        "    task_type = TaskType.FEATURE_EXTRACTION, # Important!\n",
        ")\n",
        "print(\"LORA adapters added.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Attaching QLoRA adapters...\n",
        "Unsloth: Making `model.base_model.model.encoder` require gradients\n",
        "LORA adapters added.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A key part here is the line task_type = TaskType.FEATURE_EXTRACTION which prepares the models for the embeddings loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see below how QLoRA only learns a fraction of the model's original parameters, making it feasible to run this training on regular consumer hardware instead of on massive GPU clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check how many parameters we will actually learn\n",
        "model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainable params: 1,339,392 || all params: 34,699,392 || trainable%: 3.8600\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Wrapping unsloth model with Sentence Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To use a loss for embedding models, we need the Sentence Transformers library. Below we can follow the Sentence Transformers documentation to create a custom model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we create a Transformer model. Then we manually patch in our QLoRA unsloth model and tokenizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also need to tell Sentence Transformers how to convert the model's final output into an embedding. This is known as the pooling stage. There are many pooling techniques, but it seems like mean-pooling is the rising star. Mean pooling means we take the token-wise average of the network's final activations and call that collapsed, single vector the embedding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lastly, many models include a normalization stage. This determines whether or not we scale vectors to have a uniform unit length. It's the default for sentence transformers, and in practice I've found it's saved me a lot of headache to always and only deal with normalized vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With our three modules ready, we pass them into a SentenceTransformer instance. This creates the final model that can be used by the library's Trainer class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: you can also pass in additional arguments here that would have typically be passed to the huggingface model, such as the attention implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Phew. That's a lot. Let's write an annotated function this a bit clearer and our lives a bit easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare the ST model\n",
        "def get_st_unsloth_wrapper(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        base_model_id=BASE_MODEL_ID,\n",
        "        pooling_mode=\"mean\",\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "        ):\n",
        "    print(\"Initializing Sentence Transformer modules...\")\n",
        "\n",
        "    # 1. Create the Transformer module instance\n",
        "    transformer_module = sentence_transformers.models.Transformer(\n",
        "        model_name_or_path=base_model_id,\n",
        "        max_seq_length=max_seq_length,\n",
        "    )\n",
        "\n",
        "    # 2. Replace the internal Hugging Face model with our LORA-patched Unsloth model\n",
        "    transformer_module.auto_model = model\n",
        "    transformer_module.tokenizer = tokenizer\n",
        "\n",
        "    print(f\"Manually assigned Unsloth LORA model to sentence_transformers.models.Transformer module.\")\n",
        "\n",
        "    # 3. Create the Pooling module\n",
        "    hidden_size = model.config.hidden_size\n",
        "    pooling_module = sentence_transformers.models.Pooling(\n",
        "        word_embedding_dimension=hidden_size,\n",
        "        pooling_mode=pooling_mode,\n",
        "    )\n",
        "    print(f\"Using Pooling module with mode: {pooling_mode}\")\n",
        "\n",
        "    # 4. Add the Normalize module\n",
        "    normalize_module = sentence_transformers.models.Normalize()\n",
        "    modules = [transformer_module, pooling_module, normalize_module]\n",
        "\n",
        "    # 5. Initialize SentenceTransformer with custom modules\n",
        "    sbert_model = SentenceTransformer(modules=modules)\n",
        "\n",
        "    print(f\"SentenceTransformer wrapper created with custom modules.\")\n",
        "    return sbert_model\n",
        "\n",
        "# wrap our unsloth model in Sentence Transformers\n",
        "sbert_model = get_st_unsloth_wrapper(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "        base_model_id=BASE_MODEL_ID,\n",
        "        pooling_mode=\"mean\",\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Initializing Sentence Transformer modules...\n",
        "Manually assigned Unsloth LORA model to sentence_transformers.models.Transformer module.\n",
        "Using Pooling module with mode: mean\n",
        "SentenceTransformer wrapper created with custom modules.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now focus on the most important part of this whole process: the data. The data setup below is taken from Phil Schmid's excellent guide on fine-tuning embedding models for RAG applications. We mirror this setup because it's a fun, interesting dataset and because we're mainly focused on the unsloth and QLoRA pieces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The main thing we have to do is properly format the data for the contrastive loss we will be using."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A proper deep dive into contrastive losses is far beyond the scope of this post. Here's an excellent blog post from Lilian Weng that teaches you all the basics (and then some) of these losses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from: https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/fine-tune-embedding-model-for-rag.ipynb\n",
        "\n",
        "# prepare the NVIDIA financial dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset from the hub\n",
        "dataset = load_dataset(\"philschmid/finanical-rag-embedding-dataset\", split=\"train\")\n",
        "\n",
        "# rename columns\n",
        "dataset = dataset.rename_column(\"question\", \"anchor\")\n",
        "dataset = dataset.rename_column(\"context\", \"positive\")\n",
        "\n",
        "# Add an id column to the dataset\n",
        "dataset = dataset.add_column(\"id\", range(len(dataset)))\n",
        "\n",
        "# split dataset into a 10% test set\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "# save datasets to disk\n",
        "dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\n",
        "dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Creating json from Arrow format:   0%|          | 0/7 [00:00"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Creating json from Arrow format:   0%|          | 0/1 [00:00"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "240993"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The key takeaway is that all the hard research into contrastive losses has paid off tremendously: we now have a certain kind of loss called Multiple Negatives Ranking Loss (MNRL) that makes it possible to train embeddings model with loosely, implicitly labeled data like Q&A pairs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Questions and Answers became a pair of reference (anchor) and matching (positive) vectors that should be retrieved together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For any one valid pair, the model randomly picks vectors from different training examples in the same mini-batch to use as negatives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This means all you need to start training an embeddings model is a good set of Q&A questions. With how ubiquitous and powerful this kind of data has become thanks to SFT and reasoning-based RL, you can see how we're very close to an insanely powerful data feedback loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And, we can always do some work to improve this loss by picking or mining better negative examples. But it is pretty outrageous and fortunate how quickly we can set up fine-tuning embeddings models with the MNRL loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's go ahead and define this powerful loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define the loss function\n",
        "loss = MultipleNegativesRankingLoss(sbert_model)\n",
        "print(f\"Using loss: {type(loss).__name__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Using loss: MultipleNegativesRankingLoss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next step is to define and group up all of our training arguments. A rule of thumb is that LoRA can overfit if you train for too many epochs. So we start with just a few, but this is definitely a parameter to explore."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the batch size, you should use the largest value that fits on your GPU. This is especially important for the MNRL loss since it randomly picks negative examples from the same batch. The larger the batch size, the more random negative examples it can pick from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Preparing all of our training arguments\n",
        "\n",
        "# Training Configuration\n",
        "NUM_TRAIN_EPOCHS = 4                # Start with 1 epochs\n",
        "PER_DEVICE_TRAIN_BATCH_SIZE = 512   # Adjust based on GPU VRAM and MAX_SEQ_LENGTH.\n",
        "PER_DEVICE_EVAL_BATCH_SIZE = 1024   # Can usually be higher than train batch size.\n",
        "GRADIENT_ACCUMULATION_STEPS = 1     # Only for small cards\n",
        "\n",
        "# don't repeat samples in the same batch given our loss\n",
        "batch_sampler = BatchSamplers.NO_DUPLICATES if isinstance(loss, MultipleNegativesRankingLoss) else None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The rest of the training arguments are standard for unsloth models. However, as mentioned, QLoRA adapters for encoders are a relatively unexplored space. There are likely far more optimal values, but this is a good start."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set lower for longer training runs\n",
        "LEARNING_RATE = 2e-4\n",
        "\n",
        "WARMUP_RATIO = 0.1                 # percent of warmup steps\n",
        "OPTIMIZER = \"adamw_torch_fused\"    # start with 8bit optimizer\n",
        "LR_SCHEDULER_TYPE = \"cosine\"       # schedule for the lr\n",
        "WEIGHT_DECAY = 0.1                 # Weight decay\n",
        "FP16 = not torch.cuda.is_bf16_supported() # Use FP16 if BF16 is not available\n",
        "BF16 = torch.cuda.is_bf16_supported()     # Use BF16 on supported GPUs (Ampere+) for stability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define how we'll evaluate the model. We'll also make an output directory where to save the fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set the output directory\n",
        "OUTPUT_DIR = Path(\"finetuned_embeddings\")\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# evaluation and saving\n",
        "EVAL_STEPS = 4           # evaluate every N steps\n",
        "EVAL_STRATEGY = \"steps\"\n",
        "SAVE_STEPS = 4           # save checkpoint every N steps\n",
        "SAVE_STRATEGY = \"steps\"\n",
        "SAVE_TOTAL_LIMIT = 2     # keep only the last N checkpoints\n",
        "LOGGING_STEPS = 2        # log metrics every N steps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once again, the specific evaluation setup is taken from Phil Schmid's notebook. This creates a simple evaluator that's meant to mirror how relevant embeddings should be retrieved in RAG applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load test dataset\n",
        "test_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\n",
        "train_dataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")\n",
        "corpus_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
        "\n",
        "# Convert the datasets to dictionaries\n",
        "corpus = dict(\n",
        "    zip(corpus_dataset[\"id\"], corpus_dataset[\"positive\"])\n",
        ")  # Our corpus (cid => document)\n",
        "queries = dict(\n",
        "    zip(test_dataset[\"id\"], test_dataset[\"anchor\"])\n",
        ")  # Our queries (qid => question)\n",
        "\n",
        "# Create a mapping of relevant document (1 in our case) for each query\n",
        "relevant_docs = {}  # Query ID to relevant documents (qid => set([relevant_cids])\n",
        "for q_id in queries:\n",
        "    relevant_docs[q_id] = [q_id]\n",
        "\n",
        "\n",
        "evaluator = InformationRetrievalEvaluator(\n",
        "    queries=queries,\n",
        "    corpus=corpus,\n",
        "    relevant_docs=relevant_docs,\n",
        "    score_functions={\"cosine\": cos_sim},\n",
        "    name=\"ir-eval\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With all the work done, we can now group up the training arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Defining training arguments...\")\n",
        "args = SentenceTransformerTrainingArguments(\n",
        "    # Core Training Parameters\n",
        "    output_dir=str(OUTPUT_DIR),\n",
        "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
        "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
        "    per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    optim=OPTIMIZER,\n",
        "    batch_sampler=batch_sampler,\n",
        "    fp16=FP16,\n",
        "    bf16=BF16,\n",
        "    tf32=True, # NOTE: gpu must support\n",
        "    fp16_full_eval=True,\n",
        "    # Evaluation and Saving\n",
        "    eval_strategy=EVAL_STRATEGY,\n",
        "    eval_steps=EVAL_STEPS,\n",
        "    save_strategy=SAVE_STRATEGY,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=SAVE_TOTAL_LIMIT,\n",
        "    load_best_model_at_end=True if evaluator else False,\n",
        "    metric_for_best_model=\"eval_ir-eval_cosine_ndcg@10\" if evaluator and isinstance(evaluator, InformationRetrievalEvaluator) else None,\n",
        "    greater_is_better=True,\n",
        "    # Logging and Reporting\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    report_to=\"tensorboard\",\n",
        "    run_name=f\"{BASE_MODEL_ID.split('/')[-1]}-st-finetune\",\n",
        "    seed=42,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Defining training arguments...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preparing the Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have everything we need to start training:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can wrap all of these in a SentenceTransformerTrainer and off we go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Initializing SentenceTransformerTrainer...\")\n",
        "trainer = SentenceTransformerTrainer(\n",
        "    model=sbert_model, # Pass the standard SentenceTransformer model\n",
        "    args=args,\n",
        "    train_dataset=train_dataset.select_columns([\"anchor\", \"positive\"]),\n",
        "    eval_dataset=test_dataset.select_columns([\"anchor\", \"positive\"]) if evaluator else None,\n",
        "    loss=loss,\n",
        "    evaluator=evaluator,\n",
        "    callbacks=[],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Initializing SentenceTransformerTrainer...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Computing widget examples:   0%|          | 0/1 [00:00"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Drumroll... and train the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train the unsloth embeddings\n",
        "train_res = trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
        "   \\\\   /|    Num examples = 6,300 | Num Epochs = 4 | Total steps = 52\n",
        "O^O/ \\_/ \\    Batch size per device = 512 | Gradient accumulation steps = 1\n",
        "\\        /    Data Parallel GPUs = 1 | Total batch size (512 x 1 x 1) = 512\n",
        " \"-____-\"     Trainable parameters = 1,339,392/24,008,832 (5.58% trained)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Unsloth: Will smartly offload gradients to save VRAM!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's save the model to disk so we can use it later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the fine-tuned for persistence\n",
        "sbert_model.save_pretrained(str(OUTPUT_DIR))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comparison with the original model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to evaluate the model to know if this whole process actually improved it. We'll compare it against the original model that had no QLoRA adapters. The small snippet below loads up fresh versions of both models, puts them into eval mode, and runs the evaluator on them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the baseline\n",
        "original_model = SentenceTransformer(BASE_MODEL_ID)\n",
        "original_model.eval()\n",
        "\n",
        "# load the fine-tuned model\n",
        "fine_tuned_model = SentenceTransformer(\n",
        "    args.output_dir, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "fine_tuned_model.eval()\n",
        "\n",
        "# evaluate both models\n",
        "with torch.inference_mode():\n",
        "  baselines = evaluator(original_model)\n",
        "  fine_tuned_results = evaluator(fine_tuned_model)\n",
        "\n",
        "# print their scores\n",
        "print(f\"Original model: {baselines}\")\n",
        "print(f\"Fine-tuned model: {fine_tuned_results}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
        "  warnings.warn(\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Original model: {'ir-eval_cosine_accuracy@1': 0.5985714285714285, 'ir-eval_cosine_accuracy@3': 0.7271428571428571, 'ir-eval_cosine_accuracy@5': 0.7814285714285715, 'ir-eval_cosine_accuracy@10': 0.8442857142857143, 'ir-eval_cosine_precision@1': 0.5985714285714285, 'ir-eval_cosine_precision@3': 0.24238095238095236, 'ir-eval_cosine_precision@5': 0.15628571428571425, 'ir-eval_cosine_precision@10': 0.08442857142857142, 'ir-eval_cosine_recall@1': 0.5985714285714285, 'ir-eval_cosine_recall@3': 0.7271428571428571, 'ir-eval_cosine_recall@5': 0.7814285714285715, 'ir-eval_cosine_recall@10': 0.8442857142857143, 'ir-eval_cosine_ndcg@10': 0.7169950659589105, 'ir-eval_cosine_mrr@10': 0.6768259637188209, 'ir-eval_cosine_map@100': 0.682233373628609}\n",
        "Fine-tuned model: {'ir-eval_cosine_accuracy@1': 0.65, 'ir-eval_cosine_accuracy@3': 0.7914285714285715, 'ir-eval_cosine_accuracy@5': 0.8414285714285714, 'ir-eval_cosine_accuracy@10': 0.9042857142857142, 'ir-eval_cosine_precision@1': 0.65, 'ir-eval_cosine_precision@3': 0.26380952380952377, 'ir-eval_cosine_precision@5': 0.16828571428571426, 'ir-eval_cosine_precision@10': 0.09042857142857141, 'ir-eval_cosine_recall@1': 0.65, 'ir-eval_cosine_recall@3': 0.7914285714285715, 'ir-eval_cosine_recall@5': 0.8414285714285714, 'ir-eval_cosine_recall@10': 0.9042857142857142, 'ir-eval_cosine_ndcg@10': 0.7753242341249659, 'ir-eval_cosine_mrr@10': 0.734347505668934, 'ir-eval_cosine_map@100': 0.7375553017174123}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's focus on one of the more useful retrieval metrics: NDCG@10. How did the baseline do?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# baseline result\n",
        "baselines['ir-eval_cosine_ndcg@10']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "0.7169950659589105"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's check the fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fine-tuned results\n",
        "fine_tuned_results['ir-eval_cosine_ndcg@10']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "0.7753242341249659"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great! We improved quite a bit on our baseline! Just how much better are we?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# how much better is the fine-tune?\n",
        "fine_tuned_results['ir-eval_cosine_ndcg@10'] / baselines['ir-eval_cosine_ndcg@10']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "1.0813522588025706"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We got an 8% improvement in performance on this metric. That's pretty solid gain for something that took just over 5 minutes to train. And we could fit this whole process on even an extremely low-end consumer GPU. We created a fine-tuned QLoRA embedding model that solidly outperforms its baseline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This post showed how we can fine-tune encoders with QLoRA adapters using the unsloth and Sentence Transformers libraries. We trained the models with a tiny fraction of the parameters that full fine-tuning would have otherwise taken."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "QLoRAs for encoder models are a pretty under-explored area. It is likely that many of the parameters above are not optimal, but a proper sweep and ablatement is beyond this post. I mainly wanted to share a way to reliably fine-tune encoder moders with unsloth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, encoder models are usually significantly smaller than their LLM counterparts. This gives us a nice two-for-one, where we can use incredibly large batch sizes during our fine-tuning. And because our MNR Loss picks random examples from the same batch to use as negatives, this means our loss can pick from much more varied samples."
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}